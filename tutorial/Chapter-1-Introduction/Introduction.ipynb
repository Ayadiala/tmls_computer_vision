{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Computer Vision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks Within Computer Vision\n",
    "* Image Classification\n",
    "* Object Detection\n",
    "* Image Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Networks\n",
    "<img src=\"images/cnn.jpeg\">\n",
    "\n",
    "#### Convolutional Layer\n",
    "<img src=\"images/conv.gif\">\n",
    "<img src=\"images/kernelmove.png\" width=\"300\" height=\"150\">\n",
    "\n",
    "#### Pooling Layer\n",
    "<img src=\"images/pool.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Classification\n",
    "Image classification involves assigning a label to an entire image or photograph.\n",
    "<img src=\"images/mnist.png\">\n",
    "<img src=\"images/cifar.png\" width=\"600\" height=\"350\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object Detection\n",
    "Object detection is the task of image classification with localization, although an image may contain multiple objects that require localization and classification.\n",
    "<img src=\"images/coco.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Segmentation\n",
    "Image segmentation is the task of pixel-wise classification of objects in an image.\n",
    "<img src=\"images/segmentation.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General Structure\n",
    "\n",
    "1. **Encoder**: Feature extraction through a sequence of progressively narrower and deeper filters (Think: pre-trained classification network like VGG/ResNet)\n",
    "2. **Decoder**: Progressively grows the output of the encoder into a segmentation mask resembling the pixel resolution of the input image (where most customizations happen)\n",
    "3. **Skip connections**: Long range connections to draw on features at varying spatial scales to improve model accuracy\n",
    "<img src=\"https://missinglink.ai/wp-content/uploads/2019/03/SegNet-neural-network_2x.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fully Convolutional Network (FCN) [\\[paper\\]](https://arxiv.org/abs/1411.4038)\n",
    "* Output from shallower layers have more location information.\n",
    "* Deep features can be obtained when going deeper.\n",
    "<img src=\"images/fcn.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SegNet [\\[paper\\]](https://arxiv.org/abs/1511.00561)\n",
    "* uses unpooling to upsample feature maps in decoder to use and keep high frequency details intact in the segmentation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### U-Net [\\[paper\\]](https://arxiv.org/abs/1505.04597)\n",
    "* simply concatenates the encoder feature maps to upsampled feature maps from the decoder at every stage to form a ladder like structure.\n",
    "* The architecture by its skip concatenation connections allows the decoder at each stage to learn back relevant features that are lost when pooled in the encoder.\n",
    "<img src=\"images/unet.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DeepLabv3+ [\\[paper\\]](https://arxiv.org/abs/1802.02611)\n",
    "* Atrous Convolution + Atrous Spatial Pyramid Pooling (ASPP): exploits multi-scale features by employing multiple parallel filters with different rates.\n",
    "<img src=\"images/atrous.png\">\n",
    "* Improved Decoder: Encoder features are first bilinearly upsampled by a factor of 4 and then concatenated with the corresponding low-level features from the network backbone that have the same spatial resolution\n",
    "<img src=\"images/deeplab.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PSPNet [\\[paper\\]](https://arxiv.org/abs/1612.01105)\n",
    "* Pyramid Pooling Module\n",
    "    1. CNN is used to extract feature map of the last convolutional layer\n",
    "    2. Pyramid parsing module is applied to harvest different sub-region representations, followed by upsampling and concatenation layers to form the final feature representation, which carries both local and global context information.\n",
    "    <img src=\"images/pspnet.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mask-RCNN [\\[paper\\]](https://arxiv.org/abs/1703.06870)\n",
    "* Instance Segmentation\n",
    "* FCN is added on top of CNN features of Faster R-CNN to generate a binary mask (Matrix with 1s on all locations where the pixel belongs to the object and 0s elsewhere)\n",
    "<img src=\"images/maskrcnn.jpg\">\n",
    "* R-CNN: object detection model\n",
    "    1. Generate a set of proposals for bounding boxes\n",
    "    2. Run the images in the bounding boxes through a pre-trained AlexNet and finally an SVM to see what object the image in the box is.\n",
    "    3. Run the box through bbox regression model to output tighter coordinates for the box once the object has been classified.\n",
    "    <img src=\"images/rcnn.png\">\n",
    "* Faster R-CNN: uses Regional Proposal Network (RPN) for faster candidate bounding box generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cascade Mask R-CNN [\\[paper\\]](https://arxiv.org/abs/1906.09756)\n",
    "* Same as Mask R-CNN, segmentation branch is inserted in parallel to the detection branch of Cascade R-CNN.\n",
    "* Cascade R-CNN: object detection model. Trains multiple detection heads with multiple IoU thresholds. The output of the previous detector is fed to the next as a resampling mechanism.\n",
    "<img src=\"images/cascade.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HRNet [\\[paper\\]](https://arxiv.org/abs/1908.07919)\n",
    "* Existing frameworks first encode the input image as a low-resolution representation through a subnetwork that is formed by connecting high-to-low resolution convolutions in series (e.g., ResNet, VGGNet), then recover the high-resolution representation from the encoded low-resolution representation.\n",
    "<img src=\"images/hrnet1.png\">\n",
    "* HRNet maintains high-resolution representations through parallel multi-resolution convolutions.\n",
    "<img src=\"images/hrnet2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applications\n",
    "* Self-Driving Vehicles\n",
    "* Face Detection\n",
    "* Medical Imaging\n",
    "* Video Surveillance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "There are many vision datasets out there, but since our focus will be semantic segmentation, we'll introduce the following datasets.\n",
    "\n",
    "### Cityscapes\n",
    "30 classes in urban street scenes.\n",
    "<img src=\"images/cityscape.png\">\n",
    "### PASCAL\n",
    "20 classes. The train/val data has 11,530 images containing 27,450 ROI annotated objects and 6,929 segmentations.\n",
    "<img src=\"images/pascal.png\">\n",
    "### COCO\n",
    "91 object types. More than 300k photos and 40 scene categories.\n",
    "<img src=\"images/cocodata.png\">\n",
    "### ADE20K\n",
    "3,169 object classes across 1,072 complex everyday scenes. 25k images with an average of 19.5 annotated instances and 10.5 annotated object classes per image.\n",
    "<img src=\"images/ade20k.png\">\n",
    "This is the dataset we'll be focusing our efforts on. \n",
    "\n",
    "[Here](https://github.com/mosdragon/kdd2020/tree/master/datasets) are the instructions for downloading the dataset.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
